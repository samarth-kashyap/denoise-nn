{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/shivamkc3/linear-autoencoder-using-pytorch-part-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', \n",
    "                            train=True,\n",
    "                            download=True, \n",
    "                            transform=transform)\n",
    "test_data = datasets.MNIST(root='data', \n",
    "                           train=False,\n",
    "                           download=True, \n",
    "                           transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting device to CUDA if gpu is available, else setting device to CPU\n",
    "# GPU availability can be checked using torch.cuda.is_available() function call\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test dataloaders\n",
    "num_workers = 0 # number of subprocesses to use for data loading\n",
    "batch_size = 20 # how many samples per batch to load\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f871d719ab0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (decoder): Linear(in_features=64, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        ## encoder ##\n",
    "        self.encoder = nn.Linear(784, encoding_dim)\n",
    "        ## decoder ##\n",
    "        self.decoder = nn.Linear(encoding_dim, 784)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define feedforward behavior \n",
    "        # and scale the *output* layer with a sigmoid activation function\n",
    "        \n",
    "        # pass x into encoder\n",
    "        out = F.relu(self.encoder(x))\n",
    "        # pass out into decoder\n",
    "        out = torch.sigmoid(self.decoder(out))\n",
    "        \n",
    "        return out\n",
    "\n",
    "# initialize the NN\n",
    "encoding_dim = 64\n",
    "model = Autoencoder(encoding_dim)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # specify optimizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.454182\n",
      "Epoch: 2 \tTraining Loss: 0.145932\n",
      "Epoch: 3 \tTraining Loss: 0.112928\n",
      "Epoch: 4 \tTraining Loss: 0.105667\n",
      "Epoch: 5 \tTraining Loss: 0.102746\n",
      "Epoch: 6 \tTraining Loss: 0.100951\n",
      "Epoch: 7 \tTraining Loss: 0.099765\n",
      "Epoch: 8 \tTraining Loss: 0.098769\n",
      "Epoch: 9 \tTraining Loss: 0.097834\n",
      "Epoch: 10 \tTraining Loss: 0.097146\n",
      "Epoch: 11 \tTraining Loss: 0.096612\n",
      "Epoch: 12 \tTraining Loss: 0.096172\n",
      "Epoch: 13 \tTraining Loss: 0.095826\n",
      "Epoch: 14 \tTraining Loss: 0.095511\n",
      "Epoch: 15 \tTraining Loss: 0.095220\n",
      "Epoch: 16 \tTraining Loss: 0.094970\n",
      "Epoch: 17 \tTraining Loss: 0.094753\n",
      "Epoch: 18 \tTraining Loss: 0.094556\n",
      "Epoch: 19 \tTraining Loss: 0.094380\n",
      "Epoch: 20 \tTraining Loss: 0.094221\n",
      "CPU times: user 3min 47s, sys: 2.25 s, total: 3min 49s\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        images, _ = data\n",
    "        # flatten images\n",
    "        images = images.view(images.size(0), -1)\n",
    "        images = images.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m images_flatten \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# get sample outputs\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_flatten\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# prep images for display\u001b[39;00m\n\u001b[1;32m      9\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# define feedforward behavior \u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# and scale the *output* layer with a sigmoid activation function\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# pass x into encoder\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# pass out into decoder\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(out))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images_flatten = images.view(images.size(0), -1)\n",
    "# get sample outputs\n",
    "output = model(images_flatten)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "# output is resized into a batch of images\n",
    "output = output.view(batch_size, 1, 28, 28)\n",
    "# use detach when it's an output that requires_grad\n",
    "output = output.cpu().detach().numpy()\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img.cpu().detach()), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [np.random.uniform(low=1.0, high=0.5) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5827077100621747, 0.8112183032228844]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
